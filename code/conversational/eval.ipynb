{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transfer_learning_conversational import SPECIAL_TOKENS, ATTR_TO_SPECIAL_TOKEN, add_special_tokens, build_input_from_segments, get_dataset\n",
    "from build_raw_data import get_bios\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models/dialoGPTLarge were not used when initializing GPT2LMHeadModel: ['multiple_choice_head.summary.bias', 'multiple_choice_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('./models/dialoGPTLarge')\n",
    "model = GPT2LMHeadModel.from_pretrained('./models/dialoGPTLarge')\n",
    "model.to(device)\n",
    "add_special_tokens(model,tokenizer, ATTR_TO_SPECIAL_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios = get_bios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = .7\n",
    "min_length = 5\n",
    "max_length = 50\n",
    "no_sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_filtering(logits, top_k=0., top_p=0.7, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(personality, history, tokenizer, model):\n",
    "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "    current_output = []\n",
    "    for i in range(max_length):\n",
    "        instance =  build_input_from_segments(personality, history, current_output, tokenizer, with_eos=False)\n",
    "        input_ids = torch.tensor(instance[\"input_ids\"], device=device).unsqueeze(0)\n",
    "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=device).unsqueeze(0)\n",
    "        output = model(input_ids, token_type_ids=token_type_ids)\n",
    "        logits = output.logits\n",
    "        if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
    "            logits = logits[0]\n",
    "        logits = logits[0, -1, :] / temp\n",
    "        logits = top_filtering(logits)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        prev = torch.topk(probs, 1)[1] if no_sample else torch.multinomial(probs, 1)\n",
    "        if i < min_length and prev.item() in special_tokens_ids:\n",
    "            while prev.item() in special_tokens_ids:\n",
    "                if probs.max().item() == 1:\n",
    "                    break  # avoid infinitely looping over special token\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if prev.item() in special_tokens_ids:\n",
    "            break\n",
    "        current_output.append(prev.item())\n",
    "\n",
    "    return current_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalities = pd.read_csv(\"../../data/generative_model_output.csv\")\n",
    "\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def format_input(text:str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a raw text and formats for the model input. For dialogue and personality, this means all lowercase with a space before any punctuation.\n",
    "    Args:\n",
    "        text: raw text input, cam be dialogue or bio\n",
    "    Returns:\n",
    "        text: text formatted for the model\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('([.,!?()])', r' \\1 ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    text = text.strip()\n",
    "    return(text)\n",
    "def tokenize(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n",
    "    if isinstance(obj, dict):\n",
    "        return dict((n, tokenize(o)) for n, o in obj.items())\n",
    "    return list(tokenize(o) for o in obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_bios = list(personalities['bio'] + personalities['generated_bio'])\n",
    "full_bios = [bio.lower().split(\"\\n\",1)[0] for bio in full_bios]\n",
    "full_bios = [sent_tokenizer.tokenize(bio) for bio in full_bios]\n",
    "full_bios = [[format_input(sent) for sent in sents] for sents in full_bios]\n",
    "full_bios = tokenize(full_bios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "luna is a confident and self-assured fashion designer who runs her own clothing line on the island of ginger island.she takes pride in her unique and unique style and has always been known for her ability to create unique and stunning garments for the fashion scene.luna is also a loveate and always enjoys hanging out with her customers. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "personality = random.choice(full_bios)\n",
    "raw_personality = tokenizer.decode(chain(*personality))\n",
    "name = raw_personality.split(\" \",1)[0]\n",
    "print(raw_personality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is luna and i'm a fashion designer\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "history.append(tokenizer.encode(f\"what is your name and job ?\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = sample_sequence(personality, history, tokenizer, model)\n",
    "    out_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm doing well and you?\n"
     ]
    }
   ],
   "source": [
    "history.append(tokenizer.encode(out_text))\n",
    "history.append(tokenizer.encode(\"how are you today ?\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = sample_sequence(personality, history, tokenizer, model)\n",
    "    out_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i would like to make a sweater for you\n"
     ]
    }
   ],
   "source": [
    "history.append(tokenizer.encode(out_text))\n",
    "history.append(tokenizer.encode(\"what would you like me to gather ?\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = sample_sequence(personality, history, tokenizer, model)\n",
    "    out_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for personality in full_bios:\n",
    "    raw_personality = tokenizer.decode(chain(*personality))\n",
    "    name = raw_personality.split(\" \",1)[0]\n",
    "    history = []\n",
    "    first_prompt = f\"your name is {name}, who are you ?\"\n",
    "    history.append(tokenizer.encode(first_prompt))\n",
    "    with torch.no_grad():\n",
    "        output = sample_sequence(personality, history, tokenizer, model)\n",
    "        first_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    history.append(tokenizer.encode(out_text))\n",
    "    second_prompt = \"what is going on in town today ?\"\n",
    "    history.append(tokenizer.encode(second_prompt))\n",
    "    with torch.no_grad():\n",
    "        output = sample_sequence(personality, history, tokenizer, model)\n",
    "        second_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    li.append(pd.DataFrame([[raw_personality,name,first_prompt,first_output,second_prompt,second_output]], columns=[\"Personality\",\"Name\",\"First prompt\", \"First output\", \"Second prompt\", \"Second output\"]))\n",
    "\n",
    "test_set = pd.concat(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv(\"../../data/convo_tests/convo_outputs_3_9.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f78472a4194e8f316fdf8163f9b8554bda7c889f960d27aa9d5245141252b041"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
