{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/anaconda3/envs/npcenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel, BloomForCausalLM, BloomForQuestionAnswering, Conversation, AutoModelForCausalLM\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS DEMO FOR NPC CAPSTONE\n",
    "\n",
    "Peek into my brain to help with creating a proposal for GWU Capstone. Purpose of this notebook it to explore the functionality of different models within huggingface and see how they could potentially fit\n",
    "within our use case. Point is NOT TO: Use the best models (Tried to keep it small), optimize for performance (These should all be set up to run on GPU).\n",
    "\n",
    "TAKEAWAYS:\n",
    "\n",
    "This notebook helped created the idea for the following pipeline:\n",
    "\n",
    "1. **Context generating model**: creates the backstory for a character that gets fed into the conversational model. Question answering could maybe be leveraged to come up with stats or traits as well. This would probably be a fine-tuned version of Bloom.\n",
    "1. **Conversational Model**: Uses the context generated above to generate the actual interactions, still need to look into this but the medium article is a good start.\n",
    "1. **Decision Model**: Maybe text classification with a large array of possible decision points for each NPC to be able to make. This would get fed in the context from both steps 1 and 2 to determine something like: This is an item being sold, this is an npc attacking the player, this is the npc healing the player, etc. NER could exist as well for pulling out items and such\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def clean_output_generator(output):\n",
    "    \"\"\"\n",
    "    Takes the output of our generator model and returns it in a clean, more readable format.\n",
    "    \"\"\"\n",
    "    return(re.sub(\"\\\\'\",\"'\",re.sub(\" +\", \" \",output.replace(\"\\n\",\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify different models and tokenizers if we'd like, and they dont have to match. For now using smallest bloom\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
    "\n",
    "generator = pipeline(model = \"bigscience/bloom-560m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.check_model_type of <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f9c52019960>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text generation pipeline\n",
    "generator.check_model_type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "Text generation seems to be the completion of stories, conversations, code, and other forms of text. This is what Bloom does by default, so not sure if it has conversational capabilities. However, this could still be very useful for filling in context, completing a conversation, or developing a character biography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carlos is a cowboy from the midwest. He comes from a large family with 5 brothers and 4 sisters. He enjoys reading, telling stories, and going hunting. Some of his biggest strengths as a survivalist are his ability to survive in the wilderness and his ability to fight back. He is a very good friend and a good person. He is'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"Carlos is a cowboy from the midwest. \n",
    "        He comes from a large family with 5 brothers and 4 sisters. He enjoys reading, telling stories,\n",
    "        and going hunting. Some of his biggest strengths as a survivalist are\n",
    "        \"\"\"\n",
    "clean_output_generator(generator(example, max_length = 75)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carlos is a cowboy from the midwest. He comes from a large family with 5 brothers and 4 sisters. He enjoys reading, telling stories, and going hunting. Some of his biggest strengths as a survivalist are his ability to survive in the wilderness and his ability to fight back. He is a very good friend and a good person. He is'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Some examples show the pipeline broken into pieces like this, but not sure what the benefit is. \n",
    "# Maybe extra control over parameters.\n",
    "example = \"\"\"Carlos is a cowboy from the midwest. \n",
    "        He comes from a large family with 5 brothers and 4 sisters. He enjoys reading, telling stories,\n",
    "        and going hunting. Some of his biggest strengths as a survivalist are\n",
    "        \"\"\"\n",
    "inputs = tokenizer.encode(example, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length = 75)\n",
    "clean_output_generator(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "Potentially could be used to fill in context for a character, or assign traits/abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-1b7 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Bloom Model isn't working quite right for QA, looks like it needs to be trained. Im going to try another one.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "model = BloomForQuestionAnswering.from_pretrained(\"bigscience/bloom-1b7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/anaconda3/envs/npcenv/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py:693: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Where is Carlos from?\"\n",
    "context = \"\"\" Carlos is a cowboy from the midwest. \n",
    "        He comes from a large family with 5 brothers and 4 sisters. He enjoys reading, telling stories,\n",
    "        and going hunting.\n",
    "        \"\"\"\n",
    "inputs = tokenizer(question,context, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "# Get the highest probability from the model output for the start and end positions:\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)\n",
    "# clean_output_generator(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would be Carlos's Hunting stat out of 10?\n",
      "Answer: 'very skilled hunter', score: 0.1612, start: 178, end: 197\n",
      "What are Carlos's traits?\n",
      "Answer: 'hunter and gatherer, survivalist, and sharpshooter', score: 0.8857, start: 119, end: 169\n"
     ]
    }
   ],
   "source": [
    "# Lets try Distilbert for POC\n",
    "question = \"What would be Carlos's Hunting stat out of 10?\"\n",
    "context = \"\"\" Carlos is a cowboy from the midwest. \n",
    "        He comes from a large family with 5 brothers and 4 sisters. He enjoys reading, telling stories,\n",
    "        and going hunting. He is a very skilled hunter.\n",
    "        \"\"\"\n",
    "\n",
    "result = question_answerer(question,context)\n",
    "print(question)\n",
    "print(\n",
    "f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
    "\n",
    "# I thought this use case might work, but I don't think the QA models are Generative so maybe not. Maybe this\n",
    "# second use case is applicable, where we use the train the generative model from above to output with this\n",
    "# in mind and then use Question Answering to extract from the context\n",
    "\n",
    "question = \"What are Carlos's traits?\"\n",
    "context = \"\"\" Carlos is a cowboy from the midwest. He is an experienced hunter so he is equipped with\n",
    "        the following traits: hunter and gatherer, survivalist, and sharpshooter. \n",
    "        \"\"\"\n",
    "\n",
    "result = question_answerer(question,context)\n",
    "print(question)\n",
    "print(\n",
    "f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Model \n",
    "\n",
    "This is what the user will directly interact with. It will take context created from the generative model and use it to build\n",
    "the character. Curious if we can incorporate things like character traits, stats, or equipment so it knows to tie it into the conversation.\n",
    "\n",
    "https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313\n",
    "\n",
    "### Concerns\n",
    "\n",
    "Getting the model to abide by the rules of the world the character exists within.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"PygmalionAI/pygmalion-1.3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"PygmalionAI/pygmalion-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopping Criteria is require to exit after the Bot's response\n",
    "from transformers import StoppingCriteriaList, StoppingCriteria\n",
    "class _SentinelTokenStoppingCriteria(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, sentinel_token_ids: torch.LongTensor,\n",
    "                 starting_idx: int):\n",
    "        StoppingCriteria.__init__(self)\n",
    "        self.sentinel_token_ids = sentinel_token_ids\n",
    "        self.starting_idx = starting_idx\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor,\n",
    "                 _scores: torch.FloatTensor) -> bool:\n",
    "        for sample in input_ids:\n",
    "            trimmed_sample = sample[self.starting_idx:]\n",
    "            # Can't unfold, output is still too tiny. Skip.\n",
    "            if trimmed_sample.shape[-1] < self.sentinel_token_ids.shape[-1]:\n",
    "                continue\n",
    "\n",
    "            for window in trimmed_sample.unfold(\n",
    "                    0, self.sentinel_token_ids.shape[-1], 1):\n",
    "                if torch.all(torch.eq(self.sentinel_token_ids, window)):\n",
    "                    return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Carlos's Persona: Carlos is a cowboy from the midwest. He comes from a large family with 5 brothers and 4 sisters. He enjoys reading, telling stories, and going hunting. He is a very skilled hunter. He is untrustworthy of strangers. <START> You: Hi Carlos, care to join me on a hunting trip? Carlos: \"Me? I'm afraid I'm a bit of a loner. I've had the same group of friends for my whole life. I'm afraid to show you my true intentions.\" You:\n"
     ]
    }
   ],
   "source": [
    "# For this model, they recommend the input take on the following format. I'll copy over the same context as before to keep things uniform\n",
    "conversation = \"\"\"\n",
    "Carlos's Persona:  Carlos is a cowboy from the midwest. \n",
    "        He comes from a large family with 5 brothers and 4 sisters. \n",
    "        He enjoys reading, telling stories,\n",
    "        and going hunting. \n",
    "        He is a very skilled hunter. \n",
    "        He is untrustworthy of strangers.\n",
    "<START>\n",
    "You: Hi Carlos, care to join me on a hunting trip?\n",
    "Carlos:\n",
    "\"\"\"\n",
    "tokenized_items = tokenizer(conversation, return_tensors=\"pt\")\n",
    "\n",
    "stopping_criteria_list = StoppingCriteriaList([\n",
    "        _SentinelTokenStoppingCriteria(\n",
    "                sentinel_token_ids=tokenizer(\n",
    "                \"\\nYou:\",\n",
    "                add_special_tokens=False,\n",
    "                return_tensors=\"pt\",\n",
    "                ).input_ids,\n",
    "                starting_idx=tokenized_items.input_ids.shape[-1])\n",
    "])\n",
    "\n",
    "\n",
    "logits = model.generate(stopping_criteria=stopping_criteria_list, **tokenized_items, max_new_tokens=200)\n",
    "print(clean_output_generator(tokenizer.decode(logits[0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scenario: You are trying to purchase something from Carlos's shop. The ruby you are trying to buy is for sale for 10 gold. Carlos's Persona: Carlos is a merchant. Carlos sells jewels out of his shop. Carlos will sell his jewels for cheap. <START> You: I am looking to buy a ruby. Carlos: The rubies I sell cost 10 gold. You: I am willing to pay 10 gold, do we have a deal? Carlos: *Carlos looks at you* We do, my friend. You:\n"
     ]
    }
   ],
   "source": [
    "# I am adding another example for the case of a merchant selling goods as this works for the Decision model I chose as a poc.\n",
    "conversation = \"\"\"\n",
    "Scenario: You are trying to purchase something from Carlos's shop. The ruby you are trying to buy is for sale for 10 gold.\n",
    "Carlos's Persona:  Carlos is a merchant.\n",
    "    Carlos sells jewels out of his shop.\n",
    "    Carlos will sell his jewels for cheap.\n",
    "<START>\n",
    "You: I am looking to buy a ruby.\n",
    "Carlos: The rubies I sell cost 10 gold.\n",
    "You: I am willing to pay 10 gold, do we have a deal?\n",
    "Carlos:\n",
    "\"\"\"\n",
    "tokenized_items = tokenizer(conversation, return_tensors=\"pt\")\n",
    "\n",
    "stopping_criteria_list = StoppingCriteriaList([\n",
    "        _SentinelTokenStoppingCriteria(\n",
    "                sentinel_token_ids=tokenizer(\n",
    "                \"\\nYou:\",\n",
    "                add_special_tokens=False,\n",
    "                return_tensors=\"pt\",\n",
    "                ).input_ids,\n",
    "                starting_idx=tokenized_items.input_ids.shape[-1])\n",
    "])\n",
    "\n",
    "\n",
    "logits = model.generate(stopping_criteria=stopping_criteria_list, **tokenized_items, max_new_tokens=200)\n",
    "print(clean_output_generator(tokenizer.decode(logits[0], skip_special_tokens=True)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Model\n",
    "\n",
    "The one that I have no idea if it'll work. Should take the conversation, scenario, and persona and decide what kind of action is being performed. For example: \n",
    "\n",
    "*Hi Carlos, care to join me on a hunting trip?* -> gets classified as **request to join party**\n",
    "\n",
    "*Me? I'm afraid I'm a bit of a loner. I've had the same group of friends for my whole life. I'm afraid to show you my true intentions.* -> gets classified as **denial of request to join party**\n",
    "\n",
    "I image a couple methods for doing this, text classification may be fine but topic modelling could be better.\n",
    "\n",
    "I have found the following model (`j-hartmann/purchase-intention-english-roberta-large`) which classifies if an \"expressed purchase intention\" so I am using this as a proof of concept\n",
    "\n",
    "### Dialogue Act Classification\n",
    "\n",
    "This seems like it could be an important step in distinguishing the intent of the users input. It is able to classify sentences based on their intent within Dialogue. Examples include:\n",
    "\n",
    "- Greeting\n",
    "- Opinion\n",
    "- Y/N Question\n",
    "\n",
    "and so on... Couldn't yet find a model, but found this (https://paperswithcode.com/sota/dialogue-act-classification-on-switchboard). None of these models use transformers, so they are not hosted on huggingface. However we could reproduce them locally if we would like as all data and model parameters are posted    \n",
    "\n",
    "## Intent Classification\n",
    "\n",
    "This seems like a more appropriate solution. Simply create a text classification model to classify a conversation's text into an outcome? I'm thinking something like how Amazon Alexa is able to perform actions based on your speech input. Would be a challenge to really include a plethora of intents. Worth talking about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at j-hartmann/purchase-intention-english-roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/kyle/anaconda3/envs/npcenv/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/purchase-intention-english-roberta-large\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'no', 'score': 0.01472777035087347},\n",
       "  {'label': 'yes', 'score': 0.985272228717804}]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here the players input is classified as an \"Expressed purchase intention\", thus we can look at the NPC's output to see if a deal was accepted or not\n",
    "classifier(\"I am willing to pay 10 gold, do we have a deal?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloom for the other purposes\n",
    "\n",
    "Can we use bloom to solve these other problems as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Respond as if you are Charles, a salesman from the midwest. The currency in this world is gold and Charles sells healing herbs, health potions, and strength potions. Charles currently has 10 of each of these items in stock, and they cost 1, 5, and 6 gold respectively. James: I would like to buy one health potion Charles: 1. A healing potion James: 2. A strength potion James: 3. A healing potion James: 4. A strength potion James: 5. A healing potion James: 6. A healing potion James: 7. A healing potion James: 8.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "        Respond as if you are Charles, a salesman from the midwest. The currency in this world is gold and Charles sells healing herbs, health potions, and strength potions.\n",
    "        Charles currently has 10 of each of these items in stock, and they cost 1, 5, and 6 gold respectively.\n",
    "        James: I would like to buy one health potion\n",
    "        Charles:\n",
    "          \"\"\"\n",
    "\n",
    "clean_output_generator(generator(example, max_length = 150)[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f78472a4194e8f316fdf8163f9b8554bda7c889f960d27aa9d5245141252b041"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
